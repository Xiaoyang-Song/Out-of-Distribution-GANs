{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein Distance Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from config import *\n",
    "from dataset import MNIST, CIFAR10\n",
    "from models.mnist_cnn import MNISTCNN\n",
    "from trainer import train\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training & Validation Datasets \\& Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tri_set, mnist_val_set, mnist_tri_loader, mnist_val_loader = MNIST(64, 32)\n",
    "cifar_tri_set, cifar_val_set , cifar_tri_loader, cifar_val_loader = MNIST(64, 32)\n",
    "# TODO: Show dataset statistics and sample images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a toy CNN using MNIST\n",
    "We first train a toy CNN model on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 1 | training loss: 0.3848905004084364               | training acc: 0.8981709754797441\n",
      "Epoch  # 1 | validation loss: 0.12933876934184577               | validation acc: 0.9618610223642172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:48<05:41, 48.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 2 | training loss: 0.10788713700161862               | training acc: 0.9676839019189766\n",
      "Epoch  # 2 | validation loss: 0.07408166525252431               | validation acc: 0.9759384984025559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [01:34<04:43, 47.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 3 | training loss: 0.07097873537091792               | training acc: 0.9788945895522388\n",
      "Epoch  # 3 | validation loss: 0.056490876822880794               | validation acc: 0.9819289137380192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [02:20<03:52, 46.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 4 | training loss: 0.053240704025625425               | training acc: 0.9840585021321961\n",
      "Epoch  # 4 | validation loss: 0.04801741873299366               | validation acc: 0.9838258785942492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [03:06<03:05, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 5 | training loss: 0.042140562839684204               | training acc: 0.9872734541577826\n",
      "Epoch  # 5 | validation loss: 0.04270411640732817               | validation acc: 0.9856230031948882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [03:52<02:18, 46.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 6 | training loss: 0.03438922276883361               | training acc: 0.9897554637526652\n",
      "Epoch  # 6 | validation loss: 0.03906521868500019               | validation acc: 0.9866214057507987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [04:39<01:32, 46.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 7 | training loss: 0.028456401200597992               | training acc: 0.9917044243070362\n",
      "Epoch  # 7 | validation loss: 0.03671928080491405               | validation acc: 0.9876198083067093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [05:26<00:46, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  # 8 | training loss: 0.023756266558138683               | training acc: 0.9932869136460555\n",
      "Epoch  # 8 | validation loss: 0.03605541249888951               | validation acc: 0.9877196485623003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [06:12<00:00, 46.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model = MNISTCNN().to(DEVICE)\n",
    "train(model = model, train_loader=mnist_tri_loader, val_loader=mnist_val_loader, num_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_matrix(X, Y):\n",
    "    # TODO: Change this to more generic version\n",
    "    if len(X.shape) == 2:\n",
    "        N,D = X.shape\n",
    "        M,D = Y.shape\n",
    "        return (1 - torch.eye(N, M)).to(DEVICE)\n",
    "    \n",
    "    if len(X.shape) == 3:\n",
    "        B,N,D = X.shape\n",
    "        B,M,D = Y.shape\n",
    "        return torch.unsqueeze(1 - torch.eye(N, M), 0).repeat(B, 1, 1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_2_onehot(label, C, device):\n",
    "    # transform the InD labels into one-hot vector\n",
    "    assert type(label) == torch.Tensor\n",
    "\n",
    "    size = label.shape[0]\n",
    "    if len(label.shape) == 1:\n",
    "        label = torch.unsqueeze(label, 1)\n",
    "    \n",
    "    label = label % C\n",
    "    \n",
    "    label_onehot = torch.FloatTensor(size, C).to(device)\n",
    "\n",
    "    label_onehot.zero_()\n",
    "    label_onehot.scatter_(1, label, 1)\n",
    "    return label_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_dist_test(input, target, C, device):\n",
    "    \n",
    "    test_label_onehot = label_2_onehot(target, C, device)\n",
    "    test_label_onehot = torch.unsqueeze(test_label_onehot, -1)\n",
    "    test_input = torch.unsqueeze(input, -1)\n",
    "    ##Loss value for InD samples \n",
    "    #Wasserstein-1 distance\n",
    "    # test_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1., cost=cost_matrix)\n",
    "    test_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.)\n",
    "    # ic(test_input.shape)\n",
    "    # ic(test_input[:,:,0].shape)\n",
    "    # ic(test_label_onehot[:,:,0].shape)\n",
    "    # ic(test_label_onehot.shape)\n",
    "    test_loss_value = test_loss(test_input[:,:,0], test_input, test_label_onehot[:,:,0], test_label_onehot)\n",
    "    \n",
    "    return test_loss_value\n",
    "\n",
    "def sink_dist_test_v2(input, C, device):\n",
    "    \n",
    "    all_class = torch.LongTensor([i for i in range(C)]).to(device)\n",
    "    all_class_onehot = label_2_onehot(all_class, C, device)\n",
    "    ##reshape into (B,N,D)\n",
    "    all_class_onehot = torch.unsqueeze(all_class_onehot, -1)\n",
    "    test_input = torch.unsqueeze(input, -1)\n",
    "    test_batch_size = test_input.shape[0]\n",
    "    test_loss_values = torch.zeros(test_batch_size, C).to(device)\n",
    "    # Approximate Wasserstein distance\n",
    "    test_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1., cost = cost_matrix) \n",
    "    # ic(test_batch_size)\n",
    "    for b in range(test_batch_size):\n",
    "        # ic(test_input.shape)\n",
    "        input_b = test_input[b:b+1,:,:].repeat(C, 1, 1)\n",
    "        # ic(input_b.shape)\n",
    "        # ic(input_b[0:1,:,0].shape)\n",
    "        # ic(all_class_onehot[:,:,0].shape)\n",
    "        # ic(all_class_onehot.shape)\n",
    "        # Modified the line below\n",
    "        test_loss_values[b] = torch.tensor([test_loss(input_b[c:c+1,:,0], input_b[c:c+1:,:], all_class_onehot[c:c+1,:,0], \\\n",
    "                                            all_class_onehot[c:c+1:,:]) for c in range(C)])\n",
    "    \n",
    "    return test_loss_values.min(dim=1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wasserstein Distance Toy Example Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Sanity Check\n",
    "def example_wass_loss_ind(img_id_lst):\n",
    "    wass_loss = []\n",
    "    for id in img_id_lst:\n",
    "        test_sample, test_label = mnist_tri_set.__getitem__(id)\n",
    "        # ic(test_sample.shape)\n",
    "        test_logits = model(test_sample.unsqueeze(0))\n",
    "        test_softmax = torch.softmax(test_logits, dim=-1)\n",
    "        # ic(test_softmax.shape)\n",
    "        # pred = torch.argmax(test_logits, dim=1)\n",
    "        # ic(test_softmax)\n",
    "        # ic(pred)\n",
    "        # ic(test_label)\n",
    "        # one_hot_eg = label_2_onehot(torch.tensor([test_label]), 10, DEVICE)\n",
    "        sample_wass_loss = sink_dist_test(test_softmax, torch.tensor([test_label]), 10, DEVICE)\n",
    "        wass_loss.append(sample_wass_loss)\n",
    "    return torch.tensor(wass_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mnist_tri_set: Dataset MNIST\n",
      "                       Number of datapoints: 60000\n",
      "                       Root location: ./Datasets\n",
      "                       Split: Train\n",
      "                       StandardTransform\n",
      "                   Transform: Compose(\n",
      "                                  ToTensor()\n",
      "                              )\n",
      "ic| cifar_tri_set: Dataset MNIST\n",
      "                       Number of datapoints: 60000\n",
      "                       Root location: ./Datasets\n",
      "                       Split: Train\n",
      "                       StandardTransform\n",
      "                   Transform: Compose(\n",
      "                                  ToTensor()\n",
      "                              )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./Datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(mnist_tri_set)\n",
    "ic(cifar_tri_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mean_wass_loss_ind: tensor(0.0020)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0020)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_ind_wass_loss_mc():\n",
    "    img_id_lst = torch.randint(low=0, high=60000, size=(2000,))\n",
    "    wass_loss_eg = example_wass_loss_ind(img_id_lst)\n",
    "    mean_wass_loss = torch.mean(wass_loss_eg)\n",
    "    return wass_loss_eg, mean_wass_loss\n",
    "\n",
    "wass_loss_eg_ind, mean_wass_loss_ind = run_ind_wass_loss_mc()\n",
    "ic(mean_wass_loss_ind)\n",
    "# Experiments using Monte Carlo replications\n",
    "# num_mc, ind_mc_results = 10, []\n",
    "# for mc in range(num_mc):\n",
    "#     wass_loss_eg, mean_wass_loss = run_ind_wass_loss_mc()\n",
    "#     ind_mc_results.append(mean_wass_loss)\n",
    "# ic(np.mean(ind_mc_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mean_wass_loss_ood: tensor(0.0033)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0033)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "def example_wass_loss_ood(img_id_lst):\n",
    "    wass_loss = []\n",
    "    for id in img_id_lst:\n",
    "        OOD_sample = cifar_tri_set.__getitem__(id)[0].mean(0, keepdim=True)\n",
    "        OOD_logits = model(OOD_sample.unsqueeze(0))\n",
    "        # ic(OOD_logits.shape)\n",
    "        OOD_softmax = torch.softmax(OOD_logits, dim=1)\n",
    "        # ic(OOD_softmax.shape)\n",
    "        # pred = torch.argmax(OOD_logits, dim=1)\n",
    "        # ic(OOD_softmax)\n",
    "        # ic(pred)\n",
    "        # Sanity check for OOD wasserstein distance\n",
    "        OOD_wass_loss = sink_dist_test_v2(input=OOD_softmax, C=torch.tensor(10), device=DEVICE)\n",
    "        # ic(OOD_wass_loss)\n",
    "        wass_loss.append(OOD_wass_loss)\n",
    "    return torch.tensor(wass_loss)    \n",
    "\n",
    "def run_ood_wass_loss_mc():\n",
    "    img_id_lst = torch.randint(low=0, high=60000, size=(2000,))\n",
    "    wass_loss_eg = example_wass_loss_ood(img_id_lst)\n",
    "    mean_wass_loss = torch.mean(wass_loss_eg)\n",
    "    return wass_loss_eg, mean_wass_loss\n",
    "\n",
    "wass_loss_eg_ood, mean_wass_loss_ood = run_ood_wass_loss_mc()\n",
    "ic(mean_wass_loss_ood)\n",
    "# num_mc, ood_mc_results = 10, []\n",
    "# for mc in range(num_mc):\n",
    "#     img_id_lst = torch.randint(low=0, high=60000, size=(2000,))\n",
    "#     wass_loss_eg = example_wass_loss_ood(img_id_lst)\n",
    "#     mean_wass_loss = torch.mean(wass_loss_eg)\n",
    "#     # ic(mean_wass_loss)\n",
    "#     ood_mc_results.append(mean_wass_loss)\n",
    "# ic(np.mean(ood_mc_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Toy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(wass_loss_eg_ind): 2000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(wass_loss_eg_ood): 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot in-distribution data points wass loss\n",
    "ic(len(wass_loss_eg_ind))\n",
    "plt.scatter(wass_loss_eg_ind, np.zeros(2000))\n",
    "plt.scatter(wass_loss_eg_ood, np.ones(2000))\n",
    "plt.show()\n",
    "# Plot ood data points wass loss\n",
    "ic(len(wass_loss_eg_ood))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
